{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "zigYQbFu024X",
      "metadata": {
        "id": "zigYQbFu024X"
      },
      "source": [
        "### Loading modules and data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46b2086f",
      "metadata": {
        "id": "46b2086f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scaling\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Train Test Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a5e818b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2a5e818b",
        "outputId": "39742653-64e0-4501-a38d-1efce2b713b1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-54643f41-8b1e-4ed9-8477-2af93eee7a87\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trtbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalachh</th>\n",
              "      <th>exng</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slp</th>\n",
              "      <th>caa</th>\n",
              "      <th>thall</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>63</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>145</td>\n",
              "      <td>233</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>0</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>130</td>\n",
              "      <td>250</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>187</td>\n",
              "      <td>0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>130</td>\n",
              "      <td>204</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>120</td>\n",
              "      <td>236</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>178</td>\n",
              "      <td>0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>120</td>\n",
              "      <td>354</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>163</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-54643f41-8b1e-4ed9-8477-2af93eee7a87')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-54643f41-8b1e-4ed9-8477-2af93eee7a87 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-54643f41-8b1e-4ed9-8477-2af93eee7a87');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   age  sex  cp  trtbps  chol  fbs  restecg  thalachh  exng  oldpeak  slp  \\\n",
              "0   63    1   3     145   233    1        0       150     0      2.3    0   \n",
              "1   37    1   2     130   250    0        1       187     0      3.5    0   \n",
              "2   41    0   1     130   204    0        0       172     0      1.4    2   \n",
              "3   56    1   1     120   236    0        1       178     0      0.8    2   \n",
              "4   57    0   0     120   354    0        1       163     1      0.6    2   \n",
              "\n",
              "   caa  thall  output  \n",
              "0    0      1       1  \n",
              "1    0      2       1  \n",
              "2    0      2       1  \n",
              "3    0      2       1  \n",
              "4    0      2       1  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"data/heart.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1xkcxf453nZx",
      "metadata": {
        "id": "1xkcxf453nZx"
      },
      "source": [
        "## Preprocessing data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pCV3h0hP0y9N",
      "metadata": {
        "id": "pCV3h0hP0y9N"
      },
      "source": [
        "### Detecting and dealing with outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kpGXFVyjGX_F",
      "metadata": {
        "id": "kpGXFVyjGX_F"
      },
      "source": [
        "#### Detect outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oSm85P3k4I_9",
      "metadata": {
        "id": "oSm85P3k4I_9"
      },
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "import numpy as np\n",
        "\n",
        "def present_outliers(arr):\n",
        "    vals, freq = np.unique(arr[0], return_counts=True)\n",
        "    print(vals, freq)\n",
        "    counts = {cols[i[0]]:i[1] for i in zip(vals, freq)}\n",
        "    row_ind = np.unique(arr[1])\n",
        "    print(\"Outlier count: \", counts)\n",
        "    print(\"Outlier row indices: \", row_ind)\n",
        "\n",
        "# IQR\n",
        "def detect_outliers_iqr(df, cols):\n",
        "    Q1 = np.array([np.percentile(df[cat], 25, interpolation = 'midpoint') for cat in cols])\n",
        "    Q3 = np.array([np.percentile(df[cat], 75, interpolation = 'midpoint') for cat in cols])\n",
        "    IQR = Q3 - Q1\n",
        "    u = Q3 + 1.5*IQR\n",
        "    l = Q1 - 1.5*IQR\n",
        "    upper = np.asarray(df[cols] >= (Q3+1.5*IQR)).nonzero()\n",
        "    # print(\"Upper bound:\",upper)\n",
        "    lower = np.asarray(df[cols] <= (Q1-1.5*IQR)).nonzero()\n",
        "    # print(\"Lower bound:\", lower)\n",
        "    outliers = np.concatenate((lower, upper), axis=1)\n",
        "    outliers[[0,1]] = outliers[[1,0]]\n",
        "    return outliers\n",
        "\n",
        "# Z score\n",
        "def detect_outliers_z(df, cols, thresh=3):\n",
        "    z = np.array([abs(stats.zscore(df[cat])) for cat in cols])\n",
        "    outliers = np.asarray(z>thresh).nonzero()\n",
        "    return outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V-xasex9Gjbx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-xasex9Gjbx",
        "outputId": "0183c710-0240-4ed6-f7b1-ff99fec5b4eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(array([1, 1, 2, 2, 2, 2, 3, 4, 4]), array([223, 248,  28,  85, 220, 246, 272, 204, 221]))\n",
            "[1 2 3 4] [2 4 1 2]\n",
            "Outlier count:  {'trtbps': 2, 'chol': 4, 'thalachh': 1, 'oldpeak': 2}\n",
            "Outlier row indices:  [ 28  85 204 220 221 223 246 248 272]\n"
          ]
        }
      ],
      "source": [
        "cols = ['age', 'trtbps', 'chol', 'thalachh', 'oldpeak']\n",
        "outliers_z = detect_outliers_z(df, cols)\n",
        "print(outliers_z)\n",
        "present_outliers(outliers_z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Mq_omXboblu_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mq_omXboblu_",
        "outputId": "426b9877-ade8-46fe-8ab3-2b3e86c304f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[  3   1   2   2   2   1   4   1   1   1   1   4   2   4   4   1   4   1\n",
            "    1   2   1   4   1   1   4   1   4]\n",
            " [272   8  28  85  96 101 101 110 152 195 203 204 220 220 221 223 223 228\n",
            "  241 246 248 250 260 266 291 292 295]]\n",
            "[1 2 3 4] [13  5  1  8]\n",
            "Outlier count:  {'trtbps': 13, 'chol': 5, 'thalachh': 1, 'oldpeak': 8}\n",
            "Outlier row indices:  [  8  28  85  96 101 110 152 195 203 204 220 221 223 228 241 246 248 250\n",
            " 260 266 272 291 292 295]\n"
          ]
        }
      ],
      "source": [
        "outliers_iqr = detect_outliers_iqr(df, cols)\n",
        "print(outliers_iqr)\n",
        "present_outliers(outliers_iqr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SYF2rnPhGeZI",
      "metadata": {
        "id": "SYF2rnPhGeZI"
      },
      "source": [
        "#### Remove outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-1e2NtzOGVHc",
      "metadata": {
        "id": "-1e2NtzOGVHc"
      },
      "outputs": [],
      "source": [
        "# Remove outliers\n",
        "\n",
        "# mask_iqr = np.ones(len(df), dtype=bool)\n",
        "# mask_iqr[outliers_iqr[1]] = False\n",
        "# data_clean_iqr = df[mask_iqr]\n",
        "\n",
        "# mask_z = np.ones(len(df), dtype=bool)\n",
        "# mask_z[outliers_z[1]] = False\n",
        "# data_clean_z = df[mask_z]\n",
        "# print(data_clean_iqr.shape, df.shape, data_clean_z.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FO_iCJ87Gm6B",
      "metadata": {
        "id": "FO_iCJ87Gm6B"
      },
      "source": [
        "#### Log-transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k4r6DMXcETDw",
      "metadata": {
        "id": "k4r6DMXcETDw"
      },
      "outputs": [],
      "source": [
        "from itertools import chain, combinations\n",
        "\n",
        "def powerset(iterable):\n",
        "    \"list(powerset([1,2,3])) --> [(), (1,), (2,), (3,), (1,2), (1,3), (2,3), (1,2,3)]\"\n",
        "    s = list(iterable)\n",
        "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
        "\n",
        "def optimize_log_transform(df, cols):\n",
        "    \"\"\"\n",
        "    Find categories whose transformation minimizes the number of outliers\n",
        "    \"\"\"\n",
        "    col_powerset = powerset(cols)\n",
        "    min_count = np.inf\n",
        "    log_transform_cols = []\n",
        "    for col_perm in col_powerset:\n",
        "        col_perm = list(col_perm)\n",
        "        data_clean_log = pd.DataFrame.copy(df)\n",
        "        # data_clean_log = df\n",
        "        data_clean_log[col_perm] = np.log(data_clean_log[col_perm])\n",
        "        count_outliers = len(np.unique(detect_outliers_iqr(data_clean_log, cols)))\n",
        "        # print(min_count, count_outliers, col_perm)\n",
        "        if count_outliers < min_count:\n",
        "            min_count = count_outliers\n",
        "            log_transform_cols = col_perm\n",
        "    return (min_count, log_transform_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aCa4pdKRkTOR",
      "metadata": {
        "id": "aCa4pdKRkTOR"
      },
      "source": [
        "### Other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tP5MAk8Rjz9P",
      "metadata": {
        "id": "tP5MAk8Rjz9P"
      },
      "outputs": [],
      "source": [
        "# define the columns to be encoded and scaled\n",
        "cat_cols = ['sex','exng','caa','cp','fbs','restecg','slp','thall']\n",
        "con_cols = [\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\n",
        "\n",
        "def process_data(df, cat_cols, con_cols):\n",
        "  # encoding the categorical columns\n",
        "  df = pd.get_dummies(df, columns = cat_cols, drop_first = True)\n",
        "  # defining the features and target\n",
        "  X = df.drop(['output'],axis=1)\n",
        "  y = df[['output']]\n",
        "  # instantiating the scaler\n",
        "  scaler = RobustScaler()\n",
        "  # scaling the continuous featuree\n",
        "  X[con_cols] = scaler.fit_transform(X[con_cols])\n",
        "  return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Wt2rJ_OHjsGL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "Wt2rJ_OHjsGL",
        "outputId": "50864002-dc94-4eb7-dc7c-35748ac18342"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of zero values by feature\n",
            "age          0\n",
            "trtbps       0\n",
            "chol         0\n",
            "thalachh     0\n",
            "oldpeak     99\n",
            "dtype: int64\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# untransformed data\n",
        "df_base = pd.DataFrame.copy(df)\n",
        "X, y = process_data(df_base, cat_cols, con_cols)\n",
        "\n",
        "print(\"Number of zero values by feature\")\n",
        "print((df[df==0]).count()[con_cols])\n",
        "con_cols_pos = [x for x in con_cols if x!=\"oldpeak\"] # transform only columns with positive values\n",
        "import seaborn as sns\n",
        "sns.kdeplot(data=df, x=\"oldpeak\", hue=\"output\")\n",
        "plt.title(\"Distribution of 'oldpeak'\")\n",
        "\n",
        "# log-transform data before processing\n",
        "df_log = pd.DataFrame.copy(df)\n",
        "# transform cols that minimize outlier count\n",
        "min_outlier_count, log_transform_cols = optimize_log_transform(df_log, con_cols_pos)\n",
        "df_log[log_transform_cols] = np.log(df_log[log_transform_cols])\n",
        "X_log, y_log = process_data(df_log, cat_cols, con_cols)\n",
        "\n",
        "df_log1 = pd.DataFrame.copy(df)\n",
        "# transform all cols\n",
        "df_log1[con_cols_pos] = np.log(df_log1[con_cols_pos])\n",
        "X_log1, y_log1 = process_data(df_log1, cat_cols, con_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bce4dce",
      "metadata": {
        "id": "0bce4dce"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\n",
        "X_log_train, X_log_test, y_log_train, y_log_test = train_test_split(X_log,y_log, test_size = 0.2, random_state = 42)\n",
        "X_log1_train, X_log1_test, y_log1_train, y_log1_test = train_test_split(X_log1, y_log1, test_size = 0.2, random_state = 42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dUURGIm53tTo",
      "metadata": {
        "id": "dUURGIm53tTo"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XviQ8CXX3R_S",
      "metadata": {
        "id": "XviQ8CXX3R_S"
      },
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UtaLa31miDAd",
      "metadata": {
        "id": "UtaLa31miDAd"
      },
      "source": [
        "##### Custom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i4Ku2jEXrkJD",
      "metadata": {
        "id": "i4Ku2jEXrkJD"
      },
      "outputs": [],
      "source": [
        "class SVM_():\n",
        "\n",
        "    def __init__(self, learning_rate=0.1, max_iter=1000, C = 0.1, tolerance = 1e-4):\n",
        "        self.learning_rate  = learning_rate\n",
        "        self.max_iter       = max_iter\n",
        "        self.C              = C\n",
        "        self.tolerance      = tolerance\n",
        "    \n",
        "    def fit(self, X, y, verbose=False):\n",
        "\n",
        "        self.theta = np.zeros(X.shape[1] + 1)\n",
        "        X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
        "        y = y.to_numpy()\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            dists = 1 - (y.T*(X @ self.theta)).T\n",
        "            N = X.shape[1]\n",
        "            # print(res.shape, X.shape, y.shape, dists.shape, self.theta.shape, (y.T*(X @ self.theta)).shape)\n",
        "            delta_grad = np.where(dists>0, self.theta-self.C*y*X, self.theta).sum(axis=0)\n",
        "            # print(delta_grad.shape)\n",
        "            if np.all(abs(delta_grad) >= self.tolerance):\n",
        "                self.theta -= self.learning_rate/N * delta_grad\n",
        "                if verbose:\n",
        "                    print(i, self.cost(X[:, 1:], y))\n",
        "            else:\n",
        "                break\n",
        "                \n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.sign(X @ self.theta[1:] + self.theta[0])\n",
        "\n",
        "    def cost(self, X, y):\n",
        "        dists = 1 - (y.T*(X @ self.theta[1:] + self.theta[0])).T\n",
        "        dists[dists<0] = 0\n",
        "        return 1/2 * np.dot(self.theta, self.theta) + self.C/X.shape[0] * dists.sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qJUc1ftdep6r",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJUc1ftdep6r",
        "outputId": "877f2ded-0135-4278-91e2-1ba86e005dd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.001 0.01 0.5245901639344263\n",
            "0.0010974987654930556 0.01 0.5245901639344263\n",
            "0.0012045035402587824 0.01 0.5245901639344263\n",
            "0.0013219411484660286 0.01 0.5245901639344263\n",
            "0.0014508287784959402 0.01 0.5245901639344263\n",
            "0.0015922827933410922 0.01 0.5245901639344263\n",
            "0.001747528400007683 0.01 0.5245901639344263\n",
            "0.0019179102616724887 0.01 0.5245901639344263\n",
            "0.00210490414451202 0.01 0.5245901639344263\n",
            "0.0023101297000831605 0.01 0.5245901639344263\n",
            "0.0025353644939701114 0.01 0.5245901639344263\n",
            "0.0027825594022071257 0.01 0.5245901639344263\n",
            "0.0030538555088334154 0.01 0.5245901639344263\n",
            "0.003351602650938841 0.01 0.5245901639344263\n",
            "0.0036783797718286343 0.01 0.5245901639344263\n",
            "0.004037017258596553 0.01 0.5245901639344263\n",
            "0.004430621457583882 0.01 0.5245901639344263\n",
            "0.004862601580065354 0.01 0.5245901639344263\n",
            "0.005336699231206312 0.01 0.5245901639344263\n",
            "0.005857020818056668 0.01 0.5245901639344263\n",
            "0.006428073117284319 0.01 0.5245901639344263\n",
            "0.007054802310718645 0.01 0.5245901639344263\n",
            "0.007742636826811269 0.01 0.5245901639344263\n",
            "0.008497534359086447 0.01 0.5245901639344263\n",
            "0.0093260334688322 0.01 0.5245901639344263\n",
            "0.010235310218990263 0.01 0.5245901639344263\n",
            "0.011233240329780276 0.01 0.5245901639344263\n",
            "0.012328467394420665 0.01 0.5245901639344263\n",
            "0.013530477745798075 0.01 0.5245901639344263\n",
            "0.01484968262254465 0.01 0.5245901639344263\n",
            "0.016297508346206444 0.01 0.5245901639344263\n",
            "0.01788649529057435 0.01 0.5245901639344263\n",
            "0.019630406500402715 0.01 0.5245901639344263\n",
            "0.021544346900318846 0.01 0.5245901639344263\n",
            "0.023644894126454083 0.01 0.5245901639344263\n",
            "0.025950242113997372 0.01 0.5245901639344263\n",
            "0.02848035868435802 0.01 0.5245901639344263\n",
            "0.03125715849688237 0.01 0.5245901639344263\n",
            "0.03430469286314919 0.01 0.5245901639344263\n",
            "0.037649358067924694 0.01 0.5245901639344263\n",
            "0.04132012400115339 0.01 0.5245901639344263\n",
            "0.04534878508128584 0.01 0.5245901639344263\n",
            "0.049770235643321115 0.01 0.5245901639344263\n",
            "0.05462277217684343 0.01 0.5245901639344263\n",
            "0.05994842503189412 0.01 0.5245901639344263\n",
            "0.06579332246575682 0.01 0.5245901639344263\n",
            "0.07220809018385467 0.01 0.5245901639344263\n",
            "0.07924828983539177 0.01 0.5245901639344263\n",
            "0.08697490026177834 0.01 0.5245901639344263\n",
            "0.09545484566618342 0.01 0.5245901639344263\n",
            "0.10476157527896651 0.01 0.5245901639344263\n",
            "0.11497569953977368 0.01 0.5245901639344263\n",
            "0.1261856883066021 0.01 0.5245901639344263\n",
            "0.1384886371393873 0.01 0.5245901639344263\n",
            "0.15199110829529347 0.01 0.5245901639344263\n",
            "0.1668100537200059 0.01 0.5245901639344263\n",
            "0.18307382802953698 0.01 0.5245901639344263\n",
            "0.2009233002565048 0.01 0.5245901639344263\n",
            "0.22051307399030456 0.01 0.5245901639344263\n",
            "0.24201282647943834 0.01 0.5245901639344263\n",
            "0.26560877829466867 0.01 0.5245901639344263\n",
            "0.29150530628251786 0.01 0.5245901639344263\n",
            "0.31992671377973847 0.01 0.5245901639344263\n",
            "0.35111917342151344 0.01 0.5245901639344263\n",
            "0.3853528593710531 0.01 0.5245901639344263\n",
            "0.4229242874389499 0.01 0.5245901639344263\n",
            "0.4641588833612782 0.01 0.5245901639344263\n",
            "0.5094138014816381 0.01 0.5245901639344263\n",
            "0.5590810182512228 0.01 0.5245901639344263\n",
            "0.6135907273413176 0.01 0.5245901639344263\n",
            "0.6734150657750828 0.01 0.5245901639344263\n",
            "0.7390722033525783 0.01 0.5245901639344263\n",
            "0.8111308307896873 0.01 0.5245901639344263\n",
            "0.8902150854450392 0.01 0.5245901639344263\n",
            "0.9770099572992257 0.01 0.5245901639344263\n",
            "1.0722672220103242 0.01 0.5245901639344263\n",
            "1.1768119524349991 0.01 0.5245901639344263\n",
            "1.291549665014884 0.01 0.5245901639344263\n",
            "1.4174741629268062 0.01 0.5245901639344263\n",
            "1.5556761439304723 0.01 0.5245901639344263\n",
            "1.7073526474706922 0.01 0.5245901639344263\n",
            "1.873817422860385 0.01 0.5245901639344263\n",
            "2.0565123083486534 0.01 0.5245901639344263\n",
            "2.2570197196339215 0.01 0.5245901639344263\n",
            "2.4770763559917115 0.01 0.5245901639344263\n",
            "2.718588242732943 0.01 0.5245901639344263\n",
            "2.9836472402833403 0.01 0.5245901639344263\n",
            "3.2745491628777317 0.01 0.5245901639344263\n",
            "3.5938136638046294 0.01 0.5245901639344263\n",
            "3.94420605943766 0.01 0.5245901639344263\n",
            "4.328761281083062 0.01 0.5245901639344263\n",
            "4.750810162102798 0.01 0.5245901639344263\n",
            "5.21400828799969 0.01 0.5245901639344263\n",
            "5.72236765935022 0.01 0.5245901639344263\n",
            "6.280291441834259 0.01 0.5245901639344263\n",
            "6.892612104349702 0.01 0.5245901639344263\n",
            "7.56463327554629 0.01 0.5245901639344263\n",
            "8.302175681319753 0.01 0.5245901639344263\n",
            "9.111627561154895 0.01 0.5245901639344263\n",
            "10.0 0.01 0.5245901639344263\n"
          ]
        }
      ],
      "source": [
        "c_values = np.logspace(-3, 1, 100)\n",
        "# c_values = [0.37]\n",
        "# learning_rate = np.logspace(0.001, 1, 50)\n",
        "learning_rate = [0.01]\n",
        "for c in c_values:\n",
        "        for lr in learning_rate:\n",
        "            svm = SVM_(C=c, learning_rate=lr, max_iter=10000)\n",
        "            svm.fit(X_train, y_train)\n",
        "            y_pred = svm.predict(X_test)\n",
        "            print(c, lr, accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9HAHbSe5h85u",
      "metadata": {
        "id": "9HAHbSe5h85u"
      },
      "source": [
        "##### Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "080820ee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "080820ee",
        "outputId": "609ab43c-31e4-46aa-b5f0-1f3b0c3d73ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The test accuracy score of SVM (linear) is  0.8852459016393442\n",
            "The test accuracy score of SVM (rbf) is  0.9016393442622951\n",
            "The test accuracy score of SVM (rbf) is  0.9016393442622951\n",
            "The test accuracy score of SVM (rbf) is  0.8852459016393442\n",
            "The test accuracy score of SVM (sigmoid) is  0.8688524590163934\n"
          ]
        }
      ],
      "source": [
        "for k in ['linear']:\n",
        "  # for c in np.linspace(0.360, 0.404, 30):\n",
        "  for c in [0.38]:\n",
        "    clf = SVC(kernel=k, C=c, random_state=42).fit(X_train,np.ravel(y_train))\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(f\"The test accuracy score of SVM ({k}) is \", accuracy_score(y_test, y_pred))\n",
        "for k in ['rbf']:\n",
        "  # for c in np.linspace(1.41, 1.645, 40):\n",
        "  # for c in np.linspace(0.0001,10, 50):\n",
        "  for c in [1.5]:\n",
        "   for (Xtr, Xte, ytr, yte) in [\n",
        "       (X_train, X_test, y_train, y_test),\n",
        "       (X_log_train, X_log_test, y_log_train, y_log_test),\n",
        "       (X_log1_train, X_log1_test, y_log1_train, y_log1_test)\n",
        "    ]:\n",
        "    clf = SVC(kernel=k, C=c, random_state=42).fit(Xtr,np.ravel(ytr))\n",
        "    y_pred = clf.predict(Xte)\n",
        "    print(f\"The test accuracy score of SVM ({k}) is \", accuracy_score(yte, y_pred))\n",
        "for k in ['sigmoid']:\n",
        "  # for c in np.linspace(0.97, 0.95, 100):\n",
        "  for c in [0.96]:\n",
        "    clf = SVC(kernel=k, C=c, random_state=42).fit(X_train,np.ravel(y_train))\n",
        "    y_pred = clf.predict(X_test)\n",
        "    print(f\"The test accuracy score of SVM ({k}) is \", accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "S4alZAAo1KoX",
      "metadata": {
        "id": "S4alZAAo1KoX"
      },
      "source": [
        "### Logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MupTrRYNe5_p",
      "metadata": {
        "id": "MupTrRYNe5_p"
      },
      "source": [
        "##### Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q0F3eYg_h0go",
      "metadata": {
        "id": "q0F3eYg_h0go"
      },
      "outputs": [],
      "source": [
        "class TorchLogisticRegression(torch.nn.Module):\n",
        "    def __init__(self,no_input_features):\n",
        "        super(TorchLogisticRegression,self).__init__()\n",
        "        self.layer=torch.nn.Linear(no_input_features,1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        y_predicted=torch.sigmoid(self.layer(x))\n",
        "        return y_predicted\n",
        "\n",
        "def torch_fit_predict(x_train, x_test, y_train, y_test, lr=0.01, max_iter=10000, C=0, verbose=False):\n",
        "    n_samples,n_features=X.shape\n",
        "    torch_logreg = TorchLogisticRegression(n_features)\n",
        "    criterion=torch.nn.BCELoss()                                # loss function\n",
        "    optimizer=torch.optim.SGD(torch_logreg.parameters(), lr=lr, weight_decay=C) # gradient descent\n",
        "\n",
        "    for epoch in range(max_iter):\n",
        "        y_pred=torch_logreg(x_train)\n",
        "        loss=criterion(y_pred,y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        if verbose:\n",
        "            print('epoch:', epoch+1,', loss: ', loss.item())\n",
        "\n",
        "    with torch.no_grad():\n",
        "        y_pred = torch_logreg(x_test).round()\n",
        "        print(\n",
        "            \"The test accuracy score of a Pytorch Logistic Regression is: \", \n",
        "            accuracy_score(y_test, y_pred)\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WCCIOXPOLw_x",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCCIOXPOLw_x",
        "outputId": "dd03ba13-131a-4c43-ed7c-6b3f0c93628e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The test accuracy score of a Pytorch Logistic Regression is:  0.9016393442622951\n",
            "The test accuracy score of a Pytorch Logistic Regression is:  0.8852459016393442\n",
            "The test accuracy score of a Pytorch Logistic Regression is:  0.9016393442622951\n",
            "The test accuracy score of a Pytorch Logistic Regression is:  0.9016393442622951\n",
            "The test accuracy score of a Pytorch Logistic Regression is:  0.9016393442622951\n",
            "The test accuracy score of a Pytorch Logistic Regression is:  0.9016393442622951\n",
            "The test accuracy score of a Pytorch Logistic Regression is:  0.9016393442622951\n",
            "The test accuracy score of a Pytorch Logistic Regression is:  0.8852459016393442\n",
            "The test accuracy score of a Pytorch Logistic Regression is:  0.9016393442622951\n",
            "The test accuracy score of a Pytorch Logistic Regression is:  0.9016393442622951\n"
          ]
        }
      ],
      "source": [
        "x_train_torch=torch.tensor(X_train.values).float()\n",
        "x_test_torch=torch.tensor(X_test.values).float()\n",
        "y_train_torch=torch.tensor(y_train.values).view(y_train.shape[0],1).float()\n",
        "y_test_torch=torch.tensor(y_test.values).view(y_test.shape[0],1).float()\n",
        "\n",
        "for c in np.logspace(0.1, 10, 10):\n",
        "    torch_fit_predict(x_train_torch, x_test_torch, y_train_torch, y_test_torch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_mZAjB4qe112",
      "metadata": {
        "id": "_mZAjB4qe112"
      },
      "source": [
        "##### Custom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VBclzt7kK2fv",
      "metadata": {
        "id": "VBclzt7kK2fv"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression_():\n",
        "\n",
        "    def __init__(self, learning_rate=0.1, max_iter=100, regularization='l2', C = 0.1, tolerance = 1e-4):\n",
        "        self.learning_rate  = learning_rate\n",
        "        self.max_iter       = max_iter\n",
        "        self.regularization = regularization\n",
        "        self.C              = C\n",
        "        self.tolerance      = tolerance\n",
        "    \n",
        "    def fit(self, X, y, verbose=False):\n",
        "\n",
        "        self.theta = np.zeros(X.shape[1] + 1)\n",
        "        X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
        "        y = y.to_numpy()\n",
        "        for i in range(self.max_iter):\n",
        "        \n",
        "            errors = (self.__sigmoid(X @ self.theta)) - y\n",
        "            N = X.shape[1]\n",
        "\n",
        "            if self.regularization == 'l2':\n",
        "                delta_grad = np.sum((X.T @ errors).T, axis=0) + self.C*self.theta\n",
        "                # print((X.T @ errors).shape)\n",
        "            else:\n",
        "                delta_grad = np.sum((X.T @ errors).T, axis=0)\n",
        "\n",
        "            if np.all(abs(delta_grad) >= self.tolerance):\n",
        "                self.theta -= self.learning_rate/N * delta_grad\n",
        "                if verbose:\n",
        "                    print(i, self.cost(X[:, 1:], y))\n",
        "            else:\n",
        "                break\n",
        "                \n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.round(self.__sigmoid((X @ self.theta[1:]) + self.theta[0]))\n",
        "        \n",
        "    def __sigmoid(self, X):\n",
        "        return 1/2 * (1 + np.tanh(1/2 * X))\n",
        "    \n",
        "    def cost(self, X, y):\n",
        "        m = X.shape[1]\n",
        "        h = self.__sigmoid(X @ self.theta[1:] + self.theta[0])\n",
        "        epsilon = 1e-5\n",
        "        return -(1/m)* (y.T @ np.log(h + epsilon)+(1-y).T @ np.log(1-h + epsilon)\n",
        "         + 1/2*self.C * np.dot(self.theta[1:], self.theta[1:]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hBu0GfUKej9r",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBu0GfUKej9r",
        "outputId": "7ccf9bbe-ca5d-43ff-9914-bdd661d7bc0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The test accuracy score of a custom Logistic Regression is:  0.8524590163934426\n"
          ]
        }
      ],
      "source": [
        "solvers = ['liblinear']\n",
        "penalty = ['l2']\n",
        "# c_values = np.linspace(0.001, 10, 10)\n",
        "c_values = [1.0]\n",
        "# learning_rate = np.linspace(0.001, 0.1, 4)\n",
        "learning_rate = [0.01]\n",
        "for p in penalty:\n",
        "    for c in c_values:\n",
        "        for lr in learning_rate:\n",
        "            logreg = LogisticRegression_(regularization=p, C=c, learning_rate=lr, max_iter=1000)\n",
        "            logreg.fit(X_train, y_train)\n",
        "            y_pred = logreg.predict(X_test)\n",
        "            print(\"The test accuracy score of a custom Logistic Regression is: \", accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "t8JYSU-peuXJ",
      "metadata": {
        "id": "t8JYSU-peuXJ"
      },
      "source": [
        "##### Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JrNzEgmVpqXe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrNzEgmVpqXe",
        "outputId": "9c743f7d-691d-43c3-f0ff-977c7861b350"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The test accuracy score of Logistric Regression is  0.9016393442622951\n",
            "The test accuracy score of Logistric Regression is  0.9016393442622951\n",
            "The test accuracy score of Logistric Regression is  0.9016393442622951\n",
            "The test accuracy score of Logistric Regression is  0.9016393442622951\n",
            "The test accuracy score of Logistric Regression is  0.9016393442622951\n",
            "The test accuracy score of Logistric Regression is  0.9016393442622951\n",
            "The test accuracy score of Logistric Regression is  0.9016393442622951\n",
            "The test accuracy score of Logistric Regression is  0.9016393442622951\n",
            "The test accuracy score of Logistric Regression is  0.9016393442622951\n",
            "The test accuracy score of Logistric Regression is  0.9016393442622951\n",
            "The test accuracy score of Logistric Regression is  0.9016393442622951\n",
            "The test accuracy score of Logistric Regression is  0.9016393442622951\n",
            "The test accuracy score of Logistric Regression is  0.9016393442622951\n",
            "The test accuracy score of Logistric Regression is  0.9016393442622951\n",
            "The test accuracy score of Logistric Regression is  0.9016393442622951\n",
            "The test accuracy score of Logistric Regression is  0.9016393442622951\n",
            "The test accuracy score of Logistric Regression is  0.9016393442622951\n"
          ]
        }
      ],
      "source": [
        "# solvers = ['liblinear']\n",
        "penalty = ['l1', 'l2', 'none']\n",
        "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
        "\n",
        "# check different hyperparameter values\n",
        "for p in penalty:\n",
        "    for c in c_values:\n",
        "        logreg = LogisticRegression()\n",
        "        logreg.fit(X_train, np.ravel(y_train))\n",
        "        y_pred_proba = logreg.predict_proba(X_test)\n",
        "        y_pred = np.argmax(y_pred_proba,axis=1)\n",
        "        print(\"The test accuracy score of Logistric Regression is \", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# fitting log-transformed data (all columns)\n",
        "logreg.fit(X_log_train, np.ravel(y_log_train))\n",
        "y_log_pred_proba = logreg.predict_proba(X_log_test)\n",
        "y_log_pred = np.argmax(y_log_pred_proba,axis=1)\n",
        "print(\"The test accuracy score of Logistric Regression is \", accuracy_score(y_log_test, y_log_pred))\n",
        "\n",
        "# fitting log-transformed data (selected columns)\n",
        "logreg.fit(X_log1_train, np.ravel(y_log1_train))\n",
        "y_log1_pred_proba = logreg.predict_proba(X_log1_test)\n",
        "y_log1_pred = np.argmax(y_log1_pred_proba,axis=1)\n",
        "print(\"The test accuracy score of Logistric Regression is \", accuracy_score(y_log1_test, y_log1_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aacf592f",
      "metadata": {},
      "source": [
        "### Conclusions\n",
        "\n",
        "TODO"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "zigYQbFu024X"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "c56527485b7c9dd36c8c8808345694ae9d0a642c0f00dfb6356f9a65494ceb50"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
